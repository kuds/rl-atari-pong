{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPzMmP8ma1aAk8kl4m9Ji8D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ff00efe20e9d4e1daac9564330c5cd1b": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_4e6ca0c020cb4b368613da5077dc38a9",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[35m   8%\u001b[0m \u001b[38;2;249;38;114m━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m733/9,765 \u001b[0m [ \u001b[33m0:21:03\u001b[0m < \u001b[36m4:20:00\u001b[0m , \u001b[31m1 it/s\u001b[0m ]\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">   8%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">733/9,765 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:21:03</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">4:20:00</span> , <span style=\"color: #800000; text-decoration-color: #800000\">1 it/s</span> ]\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "4e6ca0c020cb4b368613da5077dc38a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuds/rl-atari-pong/blob/main/%5BAtari%20Pong%5D%20Multi-Agent%20Reinforcement%20Learning%20PPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Atari Pong] Multi-Agent Reinforcement Learning PPO"
      ],
      "metadata": {
        "id": "ob-2hGT3jNdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig"
      ],
      "metadata": {
        "id": "Fy1WJ1R-iP4V",
        "outputId": "9f8865a2-29d8-424e-b592-d4a1139fe199",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ray[rllib] gputil"
      ],
      "metadata": {
        "id": "5Lb1vPd-bQvK",
        "outputId": "f975e739-7b78-466e-ed45-18dc82c0ef01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ray[rllib] in /usr/local/lib/python3.10/dist-packages (2.37.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (3.16.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (24.1)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (6.0.2)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (2.2.2)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (17.0.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (2024.9.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (0.1.8)\n",
            "Requirement already satisfied: gymnasium==0.28.1 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (0.28.1)\n",
            "Requirement already satisfied: lz4 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (4.3.3)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (0.24.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.13.1)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (0.12.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (13.9.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (1.26.4)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (0.0.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]) (0.20.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[rllib]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[rllib]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[rllib]) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->ray[rllib]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->ray[rllib]) (2.18.0)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (3.4.1)\n",
            "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (11.0.0)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (2.36.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (2024.9.20)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (0.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer->ray[rllib]) (1.5.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->ray[rllib]) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->ray[rllib]) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[atari]"
      ],
      "metadata": {
        "id": "9TPt_N0zax24",
        "outputId": "c067f7b2-4808-4a5c-dadb-e08323869395",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.26.4)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Requirement already satisfied: shimmy<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (0.2.1)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (0.8.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (6.4.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pettingzoo[atari]"
      ],
      "metadata": {
        "id": "7U_qTvGlj2oy",
        "outputId": "9f3943df-c0ce-4d23-919a-e7041ce9bd17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pettingzoo[atari] in /usr/local/lib/python3.10/dist-packages (1.24.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[atari]) (1.26.4)\n",
            "Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[atari]) (0.28.1)\n",
            "Requirement already satisfied: multi-agent-ale-py==0.1.11 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[atari]) (0.1.11)\n",
            "Requirement already satisfied: pygame==2.3.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[atari]) (2.3.0)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[atari]) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[atari]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[atari]) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supersuit"
      ],
      "metadata": {
        "id": "ZutDvFD8mQxS",
        "outputId": "54bf5531-de90-482a-e063-fac602eb5587",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: supersuit in /usr/local/lib/python3.10/dist-packages (3.9.3)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from supersuit) (1.26.4)\n",
            "Requirement already satisfied: gymnasium>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from supersuit) (0.28.1)\n",
            "Requirement already satisfied: tinyscaler>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from supersuit) (1.2.8)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.1->supersuit) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autorom"
      ],
      "metadata": {
        "id": "HIfWAN6EE_fK",
        "outputId": "018e79f9-ccae-42f2-c7ee-dda99eaadc95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: autorom in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!AutoROM --accept-license"
      ],
      "metadata": {
        "id": "pm9dizr3GXlS",
        "outputId": "08c8e45e-2a85-4d73-e511-228869769bdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoROM will download the Atari 2600 ROMs.\n",
            "They will be installed to:\n",
            "\t/usr/local/lib/python3.10/dist-packages/AutoROM/roms\n",
            "\t/usr/local/lib/python3.10/dist-packages/multi_agent_ale_py/roms\n",
            "\n",
            "Existing ROMs will be overwritten.\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import numpy\n",
        "import platform\n",
        "import ray\n",
        "import supersuit\n",
        "from pettingzoo.atari import pong_v3\n",
        "import torch\n",
        "from ray import tune\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
        "from ray.rllib.models import ModelCatalog\n",
        "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
        "from ray.rllib.env import PettingZooEnv\n",
        "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
        "from ray.rllib.utils.framework import try_import_torch\n",
        "from ray.rllib.utils.torch_utils import FLOAT_MAX\n",
        "from ray.tune.registry import register_env\n",
        "from importlib.metadata import version"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "By7vZmjDBGH9",
        "outputId": "f1728ca1-213b-4c7c-f81a-793439e64c90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:51: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n",
            "  from jax import xla_computation as _xla_computation\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.__version__) <\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.__version__) <\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.__version__) <\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.__version__) <\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Python Version: {platform.python_version()}\")\n",
        "print(f\"Torch Version: {version('torch')}\")\n",
        "print(f\"Is Cuda Available: {torch.cuda.is_available()}\")\n",
        "print(f\"Cuda Version: {torch.version.cuda}\")\n",
        "print(f\"Gymnasium Version: {version('gymnasium')}\")\n",
        "print(f\"Numpy Version: {version('numpy')}\")\n",
        "print(f\"PettingZoo Version: {version('pettingzoo')}\")\n",
        "print(f\"Ray Version: {version('ray')}\")\n",
        "print(f\"ALE Version: {version('ale_py')}\")\n",
        "print(f\"SuperSuit Version: {version('supersuit')}\")"
      ],
      "metadata": {
        "id": "KRrJSPuDiDN4",
        "outputId": "f9c6e5f2-da7e-4476-de85-3f75d5228366",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python Version: 3.10.12\n",
            "Torch Version: 2.4.0+cpu\n",
            "Is Cuda Available: False\n",
            "Cuda Version: None\n",
            "Gymnasium Version: 0.28.1\n",
            "Numpy Version: 1.26.4\n",
            "PettingZoo Version: 1.24.3\n",
            "Ray Version: 2.37.0\n",
            "ALE Version: 0.8.1\n",
            "SuperSuit Version: 3.9.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SvPXWTCHiO69"
      },
      "outputs": [],
      "source": [
        "# class CNNModelV2(TorchModelV2, torch.nn.Module):\n",
        "#     def __init__(self, obs_space, act_space, num_outputs, *args, **kwargs):\n",
        "#         TorchModelV2.__init__(self, obs_space, act_space, num_outputs, *args, **kwargs)\n",
        "#         torch.nn.Module.__init__(self)\n",
        "#         self.model = torch.nn.Sequential(\n",
        "#             torch.nn.Conv2d(3, 32, [8, 8], stride=(4, 4)),\n",
        "#             torch.nn.ReLU(),\n",
        "#             torch.nn.Conv2d(32, 64, [4, 4], stride=(2, 2)),\n",
        "#             torch.nn.ReLU(),\n",
        "#             torch.nn.Conv2d(64, 64, [3, 3], stride=(1, 1)),\n",
        "#             torch.nn.ReLU(),\n",
        "#             torch.nn.Flatten(),\n",
        "#             (torch.nn.Linear(3136, 512)),\n",
        "#             torch.nn.ReLU(),\n",
        "#         )\n",
        "#         self.policy_fn = torch.nn.Linear(512, num_outputs)\n",
        "#         self.value_fn = torch.nn.Linear(512, 1)\n",
        "\n",
        "#     def forward(self, input_dict, state, seq_lens):\n",
        "#         model_out = self.model(input_dict[\"obs\"].permute(0, 3, 1, 2))\n",
        "#         self._value_out = self.value_fn(model_out)\n",
        "#         return self.policy_fn(model_out), state\n",
        "\n",
        "#     def value_function(self):\n",
        "#         return self._value_out.flatten()\n",
        "\n",
        "\n",
        "# def env_creator(args):\n",
        "#     env = pong_v3.parallel_env(num_players=2)\n",
        "#     env = supersuit.color_reduction_v0(env, mode=\"B\")\n",
        "#     env = supersuit.dtype_v0(env, \"float32\")\n",
        "#     env = supersuit.resize_v1(env, x_size=84, y_size=84)\n",
        "#     env = supersuit.normalize_obs_v0(env, env_min=0, env_max=1)\n",
        "#     env = supersuit.frame_stack_v1(env, 4)\n",
        "#     return env\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     ray.init()\n",
        "\n",
        "#     env_name = \"pong_v3\"\n",
        "\n",
        "#     register_env(env_name, lambda config: ParallelPettingZooEnv(env_creator(config)))\n",
        "#     ModelCatalog.register_custom_model(\"CNNModelV2\", CNNModelV2)\n",
        "\n",
        "#     config = (\n",
        "#         PPOConfig()\n",
        "#         .environment(env=env_name, clip_actions=True)\n",
        "#         .rollouts(num_rollout_workers=1, rollout_fragment_length=128)\n",
        "#         .training(\n",
        "#             train_batch_size=512,\n",
        "#             lr=2e-5,\n",
        "#             gamma=0.99,\n",
        "#             lambda_=0.9,\n",
        "#             use_gae=True,\n",
        "#             clip_param=0.4,\n",
        "#             grad_clip=None,\n",
        "#             entropy_coeff=0.1,\n",
        "#             vf_loss_coeff=0.25,\n",
        "#             sgd_minibatch_size=64,\n",
        "#             num_sgd_iter=10,\n",
        "#         )\n",
        "#         .debugging(log_level=\"ERROR\")\n",
        "#         .framework(framework=\"torch\")\n",
        "#         .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
        "#     )\n",
        "\n",
        "#     tune.run(\n",
        "#         \"PPO\",\n",
        "#         name=\"PPO\",\n",
        "#         stop={\"timesteps_total\": 5000000 if not os.environ.get(\"CI\") else 50000},\n",
        "#         checkpoint_freq=10,\n",
        "#         storage_path=\"~/ray_results/\" + env_name,\n",
        "#         config=config.to_dict(),\n",
        "#     )\n",
        "#     ray.shutdown()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import gymnasium as gym\n",
        "\n",
        "# from ray import tune\n",
        "# from ray.rllib.algorithms.ppo import PPOConfig\n",
        "# from ray.rllib.connectors.env_to_module.frame_stacking import FrameStackingEnvToModule\n",
        "# from ray.rllib.connectors.learner.frame_stacking import FrameStackingLearner\n",
        "\n",
        "# from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
        "# from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
        "# from ray.rllib.utils.framework import try_import_torch\n",
        "# from ray.rllib.utils.torch_utils import FLOAT_MAX\n",
        "# from ray.rllib.env.wrappers.atari_wrappers import wrap_atari_for_new_api_stack\n",
        "# from ray.rllib.utils.test_utils import add_rllib_example_script_args\n",
        "\n",
        "\n",
        "# parser = add_rllib_example_script_args(\n",
        "#     default_reward=float(\"inf\"),\n",
        "#     default_timesteps=3000000,\n",
        "#     default_iters=100000000000,\n",
        "# )\n",
        "# parser.set_defaults(enable_new_api_stack=True)\n",
        "# # Use `parser` to add your own custom command line options to this script\n",
        "# # and (if needed) use their values toset up `config` below.\n",
        "# args = parser.parse_args()\n",
        "\n",
        "\n",
        "# def _make_env_to_module_connector(env):\n",
        "#     return FrameStackingEnvToModule(num_frames=4)\n",
        "\n",
        "\n",
        "# def _make_learner_connector(input_observation_space, input_action_space):\n",
        "#     return FrameStackingLearner(num_frames=4)\n",
        "\n",
        "\n",
        "# # Create a custom Atari setup (w/o the usual RLlib-hard-coded framestacking in it).\n",
        "# # We would like our frame stacking connector to do this job.\n",
        "# def _env_creator(cfg):\n",
        "#     return wrap_atari_for_new_api_stack(\n",
        "#         gym.make(args.env, **cfg, render_mode=\"rgb_array\"),\n",
        "#         # Perform frame-stacking through ConnectorV2 API.\n",
        "#         framestack=None,\n",
        "#     )\n",
        "\n",
        "\n",
        "# tune.register_env(\"env\", _env_creator)\n",
        "\n",
        "\n",
        "# config = (\n",
        "#     PPOConfig()\n",
        "#     .environment(\n",
        "#         \"env\",\n",
        "#         env_config={\n",
        "#             # Make analogous to old v4 + NoFrameskip.\n",
        "#             \"frameskip\": 1,\n",
        "#             \"full_action_space\": False,\n",
        "#             \"repeat_action_probability\": 0.0,\n",
        "#         },\n",
        "#         clip_rewards=True,\n",
        "#     )\n",
        "#     .env_runners(\n",
        "#         # num_envs_per_env_runner=5,  # 5 on old yaml example\n",
        "#         env_to_module_connector=_make_env_to_module_connector,\n",
        "#     )\n",
        "#     .training(\n",
        "#         learner_connector=_make_learner_connector,\n",
        "#         train_batch_size_per_learner=4000,  # 5000 on old yaml example\n",
        "#         minibatch_size=128,  # 500 on old yaml example\n",
        "#         lambda_=0.95,\n",
        "#         kl_coeff=0.5,\n",
        "#         clip_param=0.1,\n",
        "#         vf_clip_param=10.0,\n",
        "#         entropy_coeff=0.01,\n",
        "#         num_epochs=10,\n",
        "#         lr=0.00015 * args.num_gpus,\n",
        "#         grad_clip=100.0,\n",
        "#         grad_clip_by=\"global_norm\",\n",
        "#     )\n",
        "#     .rl_module(\n",
        "#         model_config=DefaultModelConfig(\n",
        "#             conv_filters=[[16, 4, 2], [32, 4, 2], [64, 4, 2], [128, 4, 2]],\n",
        "#             conv_activation=\"relu\",\n",
        "#             head_fcnet_hiddens=[256],\n",
        "#             vf_share_layers=True,\n",
        "#         ),\n",
        "#     )\n",
        "\n",
        "#     # (\n",
        "#     #     model_config_dict={\"fcnet_hiddens\": [32, 32]},\n",
        "#     #     rl_module_spec=RLModuleSpec(module_class=DiscreteBCTorchModule),\n",
        "#     # )\n",
        "# )\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     from ray.rllib.utils.test_utils import run_rllib_example_script_experiment\n",
        "\n",
        "#     run_rllib_example_script_experiment(config, args=args)"
      ],
      "metadata": {
        "id": "IeUDm7zUletC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import pathlib\n",
        "import time\n",
        "import uuid\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "from ray.rllib.env.wrappers.atari_wrappers import wrap_deepmind\n",
        "from ray.rllib.models import ModelCatalog\n",
        "from ray.rllib.models.modelv2 import ModelV2\n",
        "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
        "from ray.rllib.utils.annotations import override\n",
        "from ray.tune.registry import register_env\n",
        "from ray.rllib.utils.metrics import (\n",
        "    ENV_RUNNER_RESULTS,\n",
        "    EVALUATION_RESULTS,\n",
        "    EPISODE_RETURN_MEAN,\n",
        "    NUM_ENV_STEPS_SAMPLED_LIFETIME,\n",
        ")\n",
        "from torch import nn\n",
        "from tqdm.rich import tqdm\n",
        "\n",
        "\n",
        "def make_atari(env_config):\n",
        "    env = gym.make(env_config[\"name\"])\n",
        "    env = wrap_deepmind(env, 84, True, True)\n",
        "    return env\n",
        "\n",
        "\n",
        "def linear_schedule(lr, n_iterations, iteration_steps):\n",
        "    ts_lr = []\n",
        "    ts = 0\n",
        "    for iteration in range(1, n_iterations + 1):\n",
        "        frac = 1.0 - (iteration - 1.0) / n_iterations\n",
        "        ts_lr.append((ts, frac * lr))\n",
        "        ts += iteration_steps\n",
        "    return ts_lr\n",
        "\n",
        "\n",
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    torch.nn.init.orthogonal_(layer.weight, std)\n",
        "    torch.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer\n",
        "\n",
        "\n",
        "class Agent(TorchModelV2, nn.Module):\n",
        "    def __init__(\n",
        "        self, observation_space, action_space, num_outputs, model_config, name\n",
        "    ):\n",
        "        TorchModelV2.__init__(\n",
        "            self, observation_space, action_space, num_outputs, model_config, name\n",
        "        )\n",
        "        nn.Module.__init__(self)\n",
        "\n",
        "        self.network = nn.Sequential(\n",
        "            layer_init(nn.Conv2d(4, 32, 8, stride=4)),\n",
        "            nn.ReLU(),\n",
        "            layer_init(nn.Conv2d(32, 64, 4, stride=2)),\n",
        "            nn.ReLU(),\n",
        "            layer_init(nn.Conv2d(64, 64, 3, stride=1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            layer_init(nn.Linear(64 * 7 * 7, 512)),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.actor = layer_init(nn.Linear(512, num_outputs), std=0.01)\n",
        "        self.critic = layer_init(nn.Linear(512, 1), std=1)\n",
        "        self.output = None\n",
        "\n",
        "    @override(ModelV2)\n",
        "    def forward(self, input_dict, state, seq_lens):\n",
        "        x = input_dict[\"obs\"] / 255.0\n",
        "        x = torch.reshape(x, (-1, 4, 84, 84))\n",
        "        self.output = self.network(x)\n",
        "        return self.actor(self.output), []\n",
        "\n",
        "    @override(ModelV2)\n",
        "    def value_function(self):\n",
        "        assert self.output is not None, \"must call forward first!\"\n",
        "        return torch.reshape(self.critic(self.output), [-1])\n",
        "\n",
        "\n",
        "def train_atari(args):\n",
        "    total_timesteps = int(10e6)\n",
        "    lr = 2.5e-4\n",
        "    n_envs = 8\n",
        "    n_steps = 128\n",
        "    n_iterations = total_timesteps // (n_envs * n_steps)\n",
        "    lr_schedule = linear_schedule(lr, n_iterations, n_steps * n_envs)\n",
        "\n",
        "    ModelCatalog.register_custom_model(\"Agent\", Agent)\n",
        "    register_env(f\"{args.env}\", make_atari)\n",
        "\n",
        "    ppo = (\n",
        "        PPOConfig()\n",
        "        .training(\n",
        "            gamma=0.99,\n",
        "            grad_clip_by=\"global_norm\",\n",
        "            train_batch_size=128 * 8,\n",
        "            model={\"custom_model\": \"Agent\"},\n",
        "            optimizer={\"eps\": 1e-5},\n",
        "            lr_schedule=lr_schedule,\n",
        "            use_critic=True,\n",
        "            use_gae=True,\n",
        "            lambda_=0.95,\n",
        "            use_kl_loss=False,\n",
        "            kl_coeff=0.0,  # not used\n",
        "            kl_target=0.01,  # not used\n",
        "            sgd_minibatch_size=256,\n",
        "            num_sgd_iter=4,\n",
        "            shuffle_sequences=True,\n",
        "            vf_loss_coeff=0.5,\n",
        "            entropy_coeff=0.01,\n",
        "            entropy_coeff_schedule=None,\n",
        "            clip_param=0.1,\n",
        "            vf_clip_param=0.1,\n",
        "            grad_clip=0.5,\n",
        "        )\n",
        "        .environment(\n",
        "            env=f\"{args.env}\",\n",
        "            env_config={\"name\": f\"{args.env}NoFrameskip-v4\"},\n",
        "            render_env=False,\n",
        "            clip_rewards=True,\n",
        "            normalize_actions=False,\n",
        "            clip_actions=False,\n",
        "            is_atari=True,\n",
        "        )\n",
        "        .env_runners(\n",
        "            num_env_runners=8,\n",
        "            num_envs_per_env_runner=1,\n",
        "            rollout_fragment_length=128,\n",
        "            batch_mode=\"truncate_episodes\",\n",
        "            explore=True,\n",
        "            exploration_config={\"type\": \"StochasticSampling\"},\n",
        "            create_env_on_local_worker=False,\n",
        "            preprocessor_pref=None,\n",
        "            observation_filter=\"NoFilter\",\n",
        "        )\n",
        "        .framework(framework=\"torch\")\n",
        "        .evaluation(\n",
        "            evaluation_interval=None,\n",
        "            evaluation_duration=100,\n",
        "            evaluation_duration_unit=\"episodes\",\n",
        "            evaluation_config={\n",
        "                \"explore\": True,\n",
        "                \"exploration_config\": {\"type\": \"StochasticSampling\"},\n",
        "            },\n",
        "            evaluation_num_env_runners=1,\n",
        "        )\n",
        "        .debugging(seed=args.seed)\n",
        "        # .resources(\n",
        "        #     num_gpus=0.3,\n",
        "        #     num_cpus_per_worker=1,\n",
        "        #     num_gpus_per_worker=0,\n",
        "        #     num_cpus_for_local_worker=1,\n",
        "        # )\n",
        "        .reporting(\n",
        "            metrics_num_episodes_for_smoothing=100,\n",
        "            min_train_timesteps_per_iteration=128 * 8,\n",
        "            min_sample_timesteps_per_iteration=128 * 8,\n",
        "        )\n",
        "        .experimental(_disable_preprocessor_api=True, _enable_new_api_stack=False)\n",
        "        .build()\n",
        "    )\n",
        "\n",
        "    # train\n",
        "    start_time = time.time()\n",
        "    progress_data = {\"global_step\": [], \"mean_reward\": []}\n",
        "    for iteration in tqdm(range(1, n_iterations + 1)):\n",
        "        result = ppo.train()\n",
        "        #print(result)\n",
        "        rewards = result[ENV_RUNNER_RESULTS][\"hist_stats\"][\"episode_reward\"]\n",
        "        global_step = result[\"timesteps_total\"]\n",
        "        if len(rewards) > 100:\n",
        "            rewards = rewards[-100:]\n",
        "        mean_reward = np.nan if len(rewards) == 0 else float(np.mean(rewards))\n",
        "        progress_data[\"global_step\"].append(global_step)\n",
        "        progress_data[\"mean_reward\"].append(mean_reward)\n",
        "    train_end_time = time.time()\n",
        "    progress_df = pd.DataFrame(progress_data)\n",
        "    progress_df.to_csv(os.path.join(args.path, \"progress.csv\"), index=False)\n",
        "\n",
        "    # eval\n",
        "    # there seems to be an issue where rllib does not follow specified evaluation episodes of 100\n",
        "    # here we run evaluation until 100 episodes, store the final eval results as well as initial eval results\n",
        "    initial_results = []\n",
        "    results = []\n",
        "    count = 0\n",
        "    while len(results) < 100:\n",
        "        result = ppo.evaluate()\n",
        "        assert result[\"env_runner_results\"][\"episodes_this_iter\"] == len(\n",
        "            result[\"env_runner_results\"][\"hist_stats\"][\"episode_reward\"]\n",
        "        )\n",
        "        results += result[\"env_runner_results\"][\"hist_stats\"][\"episode_reward\"]\n",
        "        if count == 0:\n",
        "            initial_results += result[\"env_runner_results\"][\"hist_stats\"][\n",
        "                \"episode_reward\"\n",
        "            ]\n",
        "        count += 1\n",
        "    eval_end_time = time.time()\n",
        "    args.training_time_h = ((train_end_time - start_time) / 60) / 60\n",
        "    args.total_time_h = ((eval_end_time - start_time) / 60) / 60\n",
        "    args.eval_mean_reward = float(np.mean(results[:100]))\n",
        "    args.initial_eval_mean_reward = float(np.mean(initial_results))\n",
        "    args.initial_eval_episodes = len(initial_results)\n",
        "\n",
        "    ppo.save(args.path)\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     parser = argparse.ArgumentParser()\n",
        "#     parser.add_argument(\n",
        "#         \"-g\",\n",
        "#         \"--gpu\",\n",
        "#         type=int,\n",
        "#         help=\"Specify GPU index\",\n",
        "#         default=0,\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"-e\",\n",
        "#         \"--env\",\n",
        "#         type=str,\n",
        "#         help=\"Specify Atari environment w/o version\",\n",
        "#         default=\"Pong\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"-t\",\n",
        "#         \"--trials\",\n",
        "#         type=int,\n",
        "#         help=\"Specify number of trials\",\n",
        "#         default=5,\n",
        "#     )\n",
        "#     args = parser.parse_args()\n",
        "#     for _ in range(args.trials):\n",
        "#         args.id = uuid.uuid4().hex\n",
        "#         args.path = os.path.join(\"trials\", \"ppo\", args.env, args.id)\n",
        "#         args.seed = int(time.time())\n",
        "\n",
        "#         # create dir\n",
        "#         pathlib.Path(args.path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#         # set gpu\n",
        "#         os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{args.gpu}\"\n",
        "\n",
        "#         train_atari(args)\n",
        "\n",
        "#         # save trial info\n",
        "#         with open(os.path.join(args.path, \"info.json\"), \"w\") as f:\n",
        "#             json.dump(vars(args), f, indent=4)"
      ],
      "metadata": {
        "id": "13lI4cWaQb1J"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Object(object):\n",
        "    pass\n",
        "\n",
        "args = Object()\n",
        "args.env = \"Pong\"\n",
        "args.id = uuid.uuid4().hex\n",
        "args.path = os.path.join(\"trials\", \"ppo\", args.env, args.id)\n",
        "args.seed = int(time.time())\n",
        "train_atari(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643,
          "referenced_widgets": [
            "ff00efe20e9d4e1daac9564330c5cd1b",
            "4e6ca0c020cb4b368613da5077dc38a9"
          ]
        },
        "id": "Se7BgrMVmUuG",
        "outputId": "ef330fcf-7052-4b37-a12f-4c08fecb0687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-10-20 17:56:00,262\tWARNING deprecation.py:50 -- DeprecationWarning: `config.experimental(_enable_new_api_stack=...)` has been deprecated. Use `config.api_stack(enable_rl_module_and_learner=...)` instead. This will raise an error in the future!\n",
            "2024-10-20 17:56:00,284\tWARNING algorithm_config.py:4144 -- You have specified 1 evaluation workers, but your `evaluation_interval` is 0 or None! Therefore, evaluation will not occur automatically with each call to `Algorithm.train()`. Instead, you will have to call `Algorithm.evaluate()` manually in order to trigger an evaluation run.\n",
            "/usr/local/lib/python3.10/dist-packages/ray/rllib/algorithms/algorithm.py:555: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "`UnifiedLogger` will be removed in Ray 2.7.\n",
            "  return UnifiedLogger(config, logdir, loggers=None)\n",
            "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
            "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
            "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
            "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
            "/usr/local/lib/python3.10/dist-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
            "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
            "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
            "2024-10-20 17:56:01,805\tINFO worker.py:1786 -- Started a local Ray instance.\n",
            "\u001b[36m(pid=61960)\u001b[0m /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:51: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n",
            "\u001b[36m(pid=61960)\u001b[0m   from jax import xla_computation as _xla_computation\n",
            "\u001b[36m(pid=61873)\u001b[0m /usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "\u001b[36m(pid=61873)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\n",
            "\u001b[36m(RolloutWorker pid=61873)\u001b[0m A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
            "\u001b[36m(RolloutWorker pid=61873)\u001b[0m [Powered by Stella]\n",
            "2024-10-20 17:56:12,763\tWARNING algorithm_config.py:4144 -- You have specified 1 evaluation workers, but your `evaluation_interval` is 0 or None! Therefore, evaluation will not occur automatically with each call to `Algorithm.train()`. Instead, you will have to call `Algorithm.evaluate()` manually in order to trigger an evaluation run.\n",
            "\u001b[36m(pid=61955)\u001b[0m /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:51: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\u001b[32m [repeated 8x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
            "\u001b[36m(pid=61955)\u001b[0m   from jax import xla_computation as _xla_computation\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[36m(RolloutWorker pid=61960)\u001b[0m /usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\u001b[32m [repeated 31x across cluster]\u001b[0m\n",
            "\u001b[36m(RolloutWorker pid=61960)\u001b[0m   if (distutils.version.LooseVersion(tf.__version__) <\u001b[32m [repeated 31x across cluster]\u001b[0m\n",
            "\u001b[36m(RolloutWorker pid=61966)\u001b[0m A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[36m(RolloutWorker pid=61966)\u001b[0m [Powered by Stella]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "2024-10-20 17:56:20,456\tINFO trainable.py:161 -- Trainable.setup took 20.151 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
            "2024-10-20 17:56:20,458\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff00efe20e9d4e1daac9564330c5cd1b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-44cd737979c8>:174: TqdmExperimentalWarning: rich is experimental/alpha\n",
            "  for iteration in tqdm(range(1, n_iterations + 1)):\n",
            "2024-10-20 17:56:21,171\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "from ray import tune\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "from ray.rllib.connectors.env_to_module.frame_stacking import FrameStackingEnvToModule\n",
        "from ray.rllib.connectors.learner.frame_stacking import FrameStackingLearner\n",
        "\n",
        "# Can import just yet because Ray RLLIB hasn't updated the pip packages yet\n",
        "from ray.rllib.core.rl_module.default_model_config import DefaultModelConfig\n",
        "from ray.rllib.env.wrappers.atari_wrappers import wrap_atari_for_new_api_stack\n",
        "from ray.rllib.utils.test_utils import add_rllib_example_script_args\n",
        "\n",
        "\n",
        "parser = add_rllib_example_script_args(\n",
        "    default_reward=float(\"inf\"),\n",
        "    default_timesteps=3000000,\n",
        "    default_iters=100000000000,\n",
        ")\n",
        "parser.set_defaults(enable_new_api_stack=True)\n",
        "# Use `parser` to add your own custom command line options to this script\n",
        "# and (if needed) use their values toset up `config` below.\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "def _make_env_to_module_connector(env):\n",
        "    return FrameStackingEnvToModule(num_frames=4)\n",
        "\n",
        "\n",
        "def _make_learner_connector(input_observation_space, input_action_space):\n",
        "    return FrameStackingLearner(num_frames=4)\n",
        "\n",
        "\n",
        "# Create a custom Atari setup (w/o the usual RLlib-hard-coded framestacking in it).\n",
        "# We would like our frame stacking connector to do this job.\n",
        "def _env_creator(cfg):\n",
        "    return wrap_atari_for_new_api_stack(\n",
        "        gym.make(args.env, **cfg, render_mode=\"rgb_array\"),\n",
        "        # Perform frame-stacking through ConnectorV2 API.\n",
        "        framestack=None,\n",
        "    )\n",
        "\n",
        "\n",
        "tune.register_env(\"env\", _env_creator)\n",
        "\n",
        "\n",
        "config = (\n",
        "    PPOConfig()\n",
        "    .environment(\n",
        "        \"env\",\n",
        "        env_config={\n",
        "            # Make analogous to old v4 + NoFrameskip.\n",
        "            \"frameskip\": 1,\n",
        "            \"full_action_space\": False,\n",
        "            \"repeat_action_probability\": 0.0,\n",
        "        },\n",
        "        clip_rewards=True,\n",
        "    )\n",
        "    .env_runners(\n",
        "        # num_envs_per_env_runner=5,  # 5 on old yaml example\n",
        "        env_to_module_connector=_make_env_to_module_connector,\n",
        "    )\n",
        "    .training(\n",
        "        learner_connector=_make_learner_connector,\n",
        "        train_batch_size_per_learner=4000,  # 5000 on old yaml example\n",
        "        minibatch_size=128,  # 500 on old yaml example\n",
        "        lambda_=0.95,\n",
        "        kl_coeff=0.5,\n",
        "        clip_param=0.1,\n",
        "        vf_clip_param=10.0,\n",
        "        entropy_coeff=0.01,\n",
        "        num_epochs=10,\n",
        "        lr=0.00015 * args.num_gpus,\n",
        "        grad_clip=100.0,\n",
        "        grad_clip_by=\"global_norm\",\n",
        "    )\n",
        "    .rl_module(\n",
        "        model_config=DefaultModelConfig(\n",
        "            conv_filters=[[16, 4, 2], [32, 4, 2], [64, 4, 2], [128, 4, 2]],\n",
        "            conv_activation=\"relu\",\n",
        "            head_fcnet_hiddens=[256],\n",
        "            vf_share_layers=True,\n",
        "        ),\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from ray.rllib.utils.test_utils import run_rllib_example_script_experiment\n",
        "\n",
        "    run_rllib_example_script_experiment(config, args=args)"
      ],
      "metadata": {
        "id": "MUpPSfDqnbAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rllib/core/rl_module/default_model_config.py"
      ],
      "metadata": {
        "id": "nhamB6qWqGHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ray.rllib.core.rl_module.default_model_config import DefaultModelConfig"
      ],
      "metadata": {
        "id": "hY1SaaJBqU11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bY6_Bsf_qsWI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}