{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP+xvEkQcidego72RzZ23Iy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuds/rl-atari-pong/blob/main/%5BAtari%20Pong%5D%20Multi-Agent%20Reinforcement%20Learning%20PPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Atari Pong] Multi-Agent Reinforcement Learning PPO"
      ],
      "metadata": {
        "id": "ob-2hGT3jNdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig"
      ],
      "metadata": {
        "id": "Fy1WJ1R-iP4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ray[rllib] gputil"
      ],
      "metadata": {
        "id": "5Lb1vPd-bQvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[atari]"
      ],
      "metadata": {
        "id": "9TPt_N0zax24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pettingzoo[atari]"
      ],
      "metadata": {
        "id": "7U_qTvGlj2oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supersuit"
      ],
      "metadata": {
        "id": "ZutDvFD8mQxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autorom"
      ],
      "metadata": {
        "id": "HIfWAN6EE_fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!AutoROM --accept-license"
      ],
      "metadata": {
        "id": "pm9dizr3GXlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import numpy\n",
        "import platform\n",
        "import ray\n",
        "import supersuit\n",
        "from pettingzoo.atari import pong_v3\n",
        "import torch\n",
        "from ray import tune\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
        "from ray.rllib.models import ModelCatalog\n",
        "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
        "from ray.rllib.env import PettingZooEnv\n",
        "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
        "from ray.rllib.utils.framework import try_import_torch\n",
        "from ray.rllib.utils.torch_utils import FLOAT_MAX\n",
        "from ray.tune.registry import register_env\n",
        "from importlib.metadata import version"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "By7vZmjDBGH9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Python Version: {platform.python_version()}\")\n",
        "print(f\"Torch Version: {version('torch')}\")\n",
        "print(f\"Is Cuda Available: {torch.cuda.is_available()}\")\n",
        "print(f\"Cuda Version: {torch.version.cuda}\")\n",
        "print(f\"Gymnasium Version: {version('gymnasium')}\")\n",
        "print(f\"Numpy Version: {version('numpy')}\")\n",
        "print(f\"PettingZoo Version: {version('pettingzoo')}\")\n",
        "print(f\"Ray Version: {version('ray')}\")\n",
        "print(f\"ALE Version: {version('ale_py')}\")\n",
        "print(f\"SuperSuit Version: {version('supersuit')}\")"
      ],
      "metadata": {
        "id": "KRrJSPuDiDN4",
        "outputId": "c720699f-b2c5-48b7-a8c5-566c460f1a8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python Version: 3.10.12\n",
            "Torch Version: 2.4.0+cpu\n",
            "Is Cuda Available: False\n",
            "Cuda Version: None\n",
            "Gymnasium Version: 0.28.1\n",
            "Numpy Version: 1.26.4\n",
            "PettingZoo Version: 1.24.3\n",
            "Ray Version: 2.37.0\n",
            "ALE Version: 0.8.1\n",
            "SuperSuit Version: 3.9.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SvPXWTCHiO69"
      },
      "outputs": [],
      "source": [
        "# class CNNModelV2(TorchModelV2, torch.nn.Module):\n",
        "#     def __init__(self, obs_space, act_space, num_outputs, *args, **kwargs):\n",
        "#         TorchModelV2.__init__(self, obs_space, act_space, num_outputs, *args, **kwargs)\n",
        "#         torch.nn.Module.__init__(self)\n",
        "#         self.model = torch.nn.Sequential(\n",
        "#             torch.nn.Conv2d(3, 32, [8, 8], stride=(4, 4)),\n",
        "#             torch.nn.ReLU(),\n",
        "#             torch.nn.Conv2d(32, 64, [4, 4], stride=(2, 2)),\n",
        "#             torch.nn.ReLU(),\n",
        "#             torch.nn.Conv2d(64, 64, [3, 3], stride=(1, 1)),\n",
        "#             torch.nn.ReLU(),\n",
        "#             torch.nn.Flatten(),\n",
        "#             (torch.nn.Linear(3136, 512)),\n",
        "#             torch.nn.ReLU(),\n",
        "#         )\n",
        "#         self.policy_fn = torch.nn.Linear(512, num_outputs)\n",
        "#         self.value_fn = torch.nn.Linear(512, 1)\n",
        "\n",
        "#     def forward(self, input_dict, state, seq_lens):\n",
        "#         model_out = self.model(input_dict[\"obs\"].permute(0, 3, 1, 2))\n",
        "#         self._value_out = self.value_fn(model_out)\n",
        "#         return self.policy_fn(model_out), state\n",
        "\n",
        "#     def value_function(self):\n",
        "#         return self._value_out.flatten()\n",
        "\n",
        "\n",
        "# def env_creator(args):\n",
        "#     env = pong_v3.parallel_env(num_players=2)\n",
        "#     env = supersuit.color_reduction_v0(env, mode=\"B\")\n",
        "#     env = supersuit.dtype_v0(env, \"float32\")\n",
        "#     env = supersuit.resize_v1(env, x_size=84, y_size=84)\n",
        "#     env = supersuit.normalize_obs_v0(env, env_min=0, env_max=1)\n",
        "#     env = supersuit.frame_stack_v1(env, 4)\n",
        "#     return env\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     ray.init()\n",
        "\n",
        "#     env_name = \"pong_v3\"\n",
        "\n",
        "#     register_env(env_name, lambda config: ParallelPettingZooEnv(env_creator(config)))\n",
        "#     ModelCatalog.register_custom_model(\"CNNModelV2\", CNNModelV2)\n",
        "\n",
        "#     config = (\n",
        "#         PPOConfig()\n",
        "#         .environment(env=env_name, clip_actions=True)\n",
        "#         .rollouts(num_rollout_workers=1, rollout_fragment_length=128)\n",
        "#         .training(\n",
        "#             train_batch_size=512,\n",
        "#             lr=2e-5,\n",
        "#             gamma=0.99,\n",
        "#             lambda_=0.9,\n",
        "#             use_gae=True,\n",
        "#             clip_param=0.4,\n",
        "#             grad_clip=None,\n",
        "#             entropy_coeff=0.1,\n",
        "#             vf_loss_coeff=0.25,\n",
        "#             sgd_minibatch_size=64,\n",
        "#             num_sgd_iter=10,\n",
        "#         )\n",
        "#         .debugging(log_level=\"ERROR\")\n",
        "#         .framework(framework=\"torch\")\n",
        "#         .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
        "#     )\n",
        "\n",
        "#     tune.run(\n",
        "#         \"PPO\",\n",
        "#         name=\"PPO\",\n",
        "#         stop={\"timesteps_total\": 5000000 if not os.environ.get(\"CI\") else 50000},\n",
        "#         checkpoint_freq=10,\n",
        "#         storage_path=\"~/ray_results/\" + env_name,\n",
        "#         config=config.to_dict(),\n",
        "#     )\n",
        "#     ray.shutdown()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import gymnasium as gym\n",
        "\n",
        "# from ray import tune\n",
        "# from ray.rllib.algorithms.ppo import PPOConfig\n",
        "# from ray.rllib.connectors.env_to_module.frame_stacking import FrameStackingEnvToModule\n",
        "# from ray.rllib.connectors.learner.frame_stacking import FrameStackingLearner\n",
        "\n",
        "# from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
        "# from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
        "# from ray.rllib.utils.framework import try_import_torch\n",
        "# from ray.rllib.utils.torch_utils import FLOAT_MAX\n",
        "# from ray.rllib.env.wrappers.atari_wrappers import wrap_atari_for_new_api_stack\n",
        "# from ray.rllib.utils.test_utils import add_rllib_example_script_args\n",
        "\n",
        "\n",
        "# parser = add_rllib_example_script_args(\n",
        "#     default_reward=float(\"inf\"),\n",
        "#     default_timesteps=3000000,\n",
        "#     default_iters=100000000000,\n",
        "# )\n",
        "# parser.set_defaults(enable_new_api_stack=True)\n",
        "# # Use `parser` to add your own custom command line options to this script\n",
        "# # and (if needed) use their values toset up `config` below.\n",
        "# args = parser.parse_args()\n",
        "\n",
        "\n",
        "# def _make_env_to_module_connector(env):\n",
        "#     return FrameStackingEnvToModule(num_frames=4)\n",
        "\n",
        "\n",
        "# def _make_learner_connector(input_observation_space, input_action_space):\n",
        "#     return FrameStackingLearner(num_frames=4)\n",
        "\n",
        "\n",
        "# # Create a custom Atari setup (w/o the usual RLlib-hard-coded framestacking in it).\n",
        "# # We would like our frame stacking connector to do this job.\n",
        "# def _env_creator(cfg):\n",
        "#     return wrap_atari_for_new_api_stack(\n",
        "#         gym.make(args.env, **cfg, render_mode=\"rgb_array\"),\n",
        "#         # Perform frame-stacking through ConnectorV2 API.\n",
        "#         framestack=None,\n",
        "#     )\n",
        "\n",
        "\n",
        "# tune.register_env(\"env\", _env_creator)\n",
        "\n",
        "\n",
        "# config = (\n",
        "#     PPOConfig()\n",
        "#     .environment(\n",
        "#         \"env\",\n",
        "#         env_config={\n",
        "#             # Make analogous to old v4 + NoFrameskip.\n",
        "#             \"frameskip\": 1,\n",
        "#             \"full_action_space\": False,\n",
        "#             \"repeat_action_probability\": 0.0,\n",
        "#         },\n",
        "#         clip_rewards=True,\n",
        "#     )\n",
        "#     .env_runners(\n",
        "#         # num_envs_per_env_runner=5,  # 5 on old yaml example\n",
        "#         env_to_module_connector=_make_env_to_module_connector,\n",
        "#     )\n",
        "#     .training(\n",
        "#         learner_connector=_make_learner_connector,\n",
        "#         train_batch_size_per_learner=4000,  # 5000 on old yaml example\n",
        "#         minibatch_size=128,  # 500 on old yaml example\n",
        "#         lambda_=0.95,\n",
        "#         kl_coeff=0.5,\n",
        "#         clip_param=0.1,\n",
        "#         vf_clip_param=10.0,\n",
        "#         entropy_coeff=0.01,\n",
        "#         num_epochs=10,\n",
        "#         lr=0.00015 * args.num_gpus,\n",
        "#         grad_clip=100.0,\n",
        "#         grad_clip_by=\"global_norm\",\n",
        "#     )\n",
        "#     .rl_module(\n",
        "#         model_config=DefaultModelConfig(\n",
        "#             conv_filters=[[16, 4, 2], [32, 4, 2], [64, 4, 2], [128, 4, 2]],\n",
        "#             conv_activation=\"relu\",\n",
        "#             head_fcnet_hiddens=[256],\n",
        "#             vf_share_layers=True,\n",
        "#         ),\n",
        "#     )\n",
        "\n",
        "#     # (\n",
        "#     #     model_config_dict={\"fcnet_hiddens\": [32, 32]},\n",
        "#     #     rl_module_spec=RLModuleSpec(module_class=DiscreteBCTorchModule),\n",
        "#     # )\n",
        "# )\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     from ray.rllib.utils.test_utils import run_rllib_example_script_experiment\n",
        "\n",
        "#     run_rllib_example_script_experiment(config, args=args)"
      ],
      "metadata": {
        "id": "IeUDm7zUletC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import pathlib\n",
        "import time\n",
        "import uuid\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "from ray.rllib.env.wrappers.atari_wrappers import wrap_deepmind\n",
        "from ray.rllib.models import ModelCatalog\n",
        "from ray.rllib.models.modelv2 import ModelV2\n",
        "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
        "from ray.rllib.utils.annotations import override\n",
        "from ray.tune.registry import register_env\n",
        "from ray.rllib.utils.metrics import (\n",
        "    ENV_RUNNER_RESULTS,\n",
        "    EVALUATION_RESULTS,\n",
        "    EPISODE_RETURN_MEAN,\n",
        "    NUM_ENV_STEPS_SAMPLED_LIFETIME,\n",
        ")\n",
        "from torch import nn\n",
        "from tqdm.rich import tqdm\n",
        "\n",
        "\n",
        "def make_atari(env_config):\n",
        "    env = gym.make(env_config[\"name\"])\n",
        "    env = wrap_deepmind(env, 84, True, True)\n",
        "    return env\n",
        "\n",
        "\n",
        "def linear_schedule(lr, n_iterations, iteration_steps):\n",
        "    ts_lr = []\n",
        "    ts = 0\n",
        "    for iteration in range(1, n_iterations + 1):\n",
        "        frac = 1.0 - (iteration - 1.0) / n_iterations\n",
        "        ts_lr.append((ts, frac * lr))\n",
        "        ts += iteration_steps\n",
        "    return ts_lr\n",
        "\n",
        "\n",
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    torch.nn.init.orthogonal_(layer.weight, std)\n",
        "    torch.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer\n",
        "\n",
        "\n",
        "class Agent(TorchModelV2, nn.Module):\n",
        "    def __init__(\n",
        "        self, observation_space, action_space, num_outputs, model_config, name\n",
        "    ):\n",
        "        TorchModelV2.__init__(\n",
        "            self, observation_space, action_space, num_outputs, model_config, name\n",
        "        )\n",
        "        nn.Module.__init__(self)\n",
        "\n",
        "        self.network = nn.Sequential(\n",
        "            layer_init(nn.Conv2d(4, 32, 8, stride=4)),\n",
        "            nn.ReLU(),\n",
        "            layer_init(nn.Conv2d(32, 64, 4, stride=2)),\n",
        "            nn.ReLU(),\n",
        "            layer_init(nn.Conv2d(64, 64, 3, stride=1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            layer_init(nn.Linear(64 * 7 * 7, 512)),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.actor = layer_init(nn.Linear(512, num_outputs), std=0.01)\n",
        "        self.critic = layer_init(nn.Linear(512, 1), std=1)\n",
        "        self.output = None\n",
        "\n",
        "    @override(ModelV2)\n",
        "    def forward(self, input_dict, state, seq_lens):\n",
        "        x = input_dict[\"obs\"] / 255.0\n",
        "        x = torch.reshape(x, (-1, 4, 84, 84))\n",
        "        self.output = self.network(x)\n",
        "        return self.actor(self.output), []\n",
        "\n",
        "    @override(ModelV2)\n",
        "    def value_function(self):\n",
        "        assert self.output is not None, \"must call forward first!\"\n",
        "        return torch.reshape(self.critic(self.output), [-1])\n",
        "\n",
        "\n",
        "def train_atari(args):\n",
        "    total_timesteps = int(10e6)\n",
        "    lr = 2.5e-4\n",
        "    n_envs = 8\n",
        "    n_steps = 128\n",
        "    n_iterations = total_timesteps // (n_envs * n_steps)\n",
        "    lr_schedule = linear_schedule(lr, n_iterations, n_steps * n_envs)\n",
        "\n",
        "    ModelCatalog.register_custom_model(\"Agent\", Agent)\n",
        "    register_env(f\"{args.env}\", make_atari)\n",
        "\n",
        "    ppo = (\n",
        "        PPOConfig()\n",
        "        .training(\n",
        "            gamma=0.99,\n",
        "            grad_clip_by=\"global_norm\",\n",
        "            train_batch_size=128 * 8,\n",
        "            model={\"custom_model\": \"Agent\"},\n",
        "            optimizer={\"eps\": 1e-5},\n",
        "            lr_schedule=lr_schedule,\n",
        "            use_critic=True,\n",
        "            use_gae=True,\n",
        "            lambda_=0.95,\n",
        "            use_kl_loss=False,\n",
        "            kl_coeff=0.0,  # not used\n",
        "            kl_target=0.01,  # not used\n",
        "            sgd_minibatch_size=256,\n",
        "            num_sgd_iter=4,\n",
        "            shuffle_sequences=True,\n",
        "            vf_loss_coeff=0.5,\n",
        "            entropy_coeff=0.01,\n",
        "            entropy_coeff_schedule=None,\n",
        "            clip_param=0.1,\n",
        "            vf_clip_param=0.1,\n",
        "            grad_clip=0.5,\n",
        "        )\n",
        "        .environment(\n",
        "            env=f\"{args.env}\",\n",
        "            env_config={\"name\": f\"{args.env}NoFrameskip-v4\"},\n",
        "            render_env=False,\n",
        "            clip_rewards=True,\n",
        "            normalize_actions=False,\n",
        "            clip_actions=False,\n",
        "            is_atari=True,\n",
        "        )\n",
        "        .env_runners(\n",
        "            num_env_runners=8,\n",
        "            num_envs_per_env_runner=1,\n",
        "            rollout_fragment_length=128,\n",
        "            batch_mode=\"truncate_episodes\",\n",
        "            explore=True,\n",
        "            exploration_config={\"type\": \"StochasticSampling\"},\n",
        "            create_env_on_local_worker=False,\n",
        "            preprocessor_pref=None,\n",
        "            observation_filter=\"NoFilter\",\n",
        "        )\n",
        "        .framework(framework=\"torch\")\n",
        "        .evaluation(\n",
        "            evaluation_interval=None,\n",
        "            evaluation_duration=100,\n",
        "            evaluation_duration_unit=\"episodes\",\n",
        "            evaluation_config={\n",
        "                \"explore\": True,\n",
        "                \"exploration_config\": {\"type\": \"StochasticSampling\"},\n",
        "            },\n",
        "            evaluation_num_env_runners=1,\n",
        "        )\n",
        "        .debugging(seed=args.seed)\n",
        "        # .resources(\n",
        "        #     num_gpus=0.3,\n",
        "        #     num_cpus_per_worker=1,\n",
        "        #     num_gpus_per_worker=0,\n",
        "        #     num_cpus_for_local_worker=1,\n",
        "        # )\n",
        "        .reporting(\n",
        "            metrics_num_episodes_for_smoothing=100,\n",
        "            min_train_timesteps_per_iteration=128 * 8,\n",
        "            min_sample_timesteps_per_iteration=128 * 8,\n",
        "        )\n",
        "        .experimental(_disable_preprocessor_api=True, _enable_new_api_stack=False)\n",
        "        .build()\n",
        "    )\n",
        "\n",
        "    # train\n",
        "    start_time = time.time()\n",
        "    progress_data = {\"global_step\": [], \"mean_reward\": []}\n",
        "    for iteration in tqdm(range(1, n_iterations + 1)):\n",
        "        result = ppo.train()\n",
        "        #print(result)\n",
        "        rewards = result[ENV_RUNNER_RESULTS][\"hist_stats\"][\"episode_reward\"]\n",
        "        global_step = result[\"timesteps_total\"]\n",
        "        if len(rewards) > 100:\n",
        "            rewards = rewards[-100:]\n",
        "        mean_reward = np.nan if len(rewards) == 0 else float(np.mean(rewards))\n",
        "        progress_data[\"global_step\"].append(global_step)\n",
        "        progress_data[\"mean_reward\"].append(mean_reward)\n",
        "    train_end_time = time.time()\n",
        "    progress_df = pd.DataFrame(progress_data)\n",
        "    progress_df.to_csv(os.path.join(args.path, \"progress.csv\"), index=False)\n",
        "\n",
        "    # eval\n",
        "    # there seems to be an issue where rllib does not follow specified evaluation episodes of 100\n",
        "    # here we run evaluation until 100 episodes, store the final eval results as well as initial eval results\n",
        "    initial_results = []\n",
        "    results = []\n",
        "    count = 0\n",
        "    while len(results) < 100:\n",
        "        result = ppo.evaluate()\n",
        "        assert result[ENV_RUNNER_RESULTS][\"episodes_this_iter\"] == len(\n",
        "            result[ENV_RUNNER_RESULTS][\"hist_stats\"][\"episode_reward\"]\n",
        "        )\n",
        "        results += result[ENV_RUNNER_RESULTS][\"hist_stats\"][\"episode_reward\"]\n",
        "        if count == 0:\n",
        "            initial_results += result[ENV_RUNNER_RESULTS][\"hist_stats\"][\n",
        "                \"episode_reward\"\n",
        "            ]\n",
        "        count += 1\n",
        "    eval_end_time = time.time()\n",
        "    args.training_time_h = ((train_end_time - start_time) / 60) / 60\n",
        "    args.total_time_h = ((eval_end_time - start_time) / 60) / 60\n",
        "    args.eval_mean_reward = float(np.mean(results[:100]))\n",
        "    args.initial_eval_mean_reward = float(np.mean(initial_results))\n",
        "    args.initial_eval_episodes = len(initial_results)\n",
        "\n",
        "    ppo.save(args.path)\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     parser = argparse.ArgumentParser()\n",
        "#     parser.add_argument(\n",
        "#         \"-g\",\n",
        "#         \"--gpu\",\n",
        "#         type=int,\n",
        "#         help=\"Specify GPU index\",\n",
        "#         default=0,\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"-e\",\n",
        "#         \"--env\",\n",
        "#         type=str,\n",
        "#         help=\"Specify Atari environment w/o version\",\n",
        "#         default=\"Pong\",\n",
        "#     )\n",
        "#     parser.add_argument(\n",
        "#         \"-t\",\n",
        "#         \"--trials\",\n",
        "#         type=int,\n",
        "#         help=\"Specify number of trials\",\n",
        "#         default=5,\n",
        "#     )\n",
        "#     args = parser.parse_args()\n",
        "#     for _ in range(args.trials):\n",
        "#         args.id = uuid.uuid4().hex\n",
        "#         args.path = os.path.join(\"trials\", \"ppo\", args.env, args.id)\n",
        "#         args.seed = int(time.time())\n",
        "\n",
        "#         # create dir\n",
        "#         pathlib.Path(args.path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#         # set gpu\n",
        "#         os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{args.gpu}\"\n",
        "\n",
        "#         train_atari(args)\n",
        "\n",
        "#         # save trial info\n",
        "#         with open(os.path.join(args.path, \"info.json\"), \"w\") as f:\n",
        "#             json.dump(vars(args), f, indent=4)"
      ],
      "metadata": {
        "id": "13lI4cWaQb1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60319307-ea6f-48c6-c410-3abab81b7a83"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.__version__) <\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.__version__) <\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.__version__) <\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Object(object):\n",
        "    pass\n",
        "\n",
        "args = Object()\n",
        "args.env = \"Pong\"\n",
        "args.id = uuid.uuid4().hex\n",
        "args.path = os.path.join(\"trials\", \"ppo\", args.env, args.id)\n",
        "args.seed = int(time.time())\n",
        "\n",
        "pathlib.Path(args.path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#save trial info\n",
        "with open(os.path.join(args.path, \"info.json\"), \"w\") as f:\n",
        "  json.dump(vars(args), f, indent=4)\n",
        "\n",
        "train_atari(args)"
      ],
      "metadata": {
        "id": "Se7BgrMVmUuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "from ray import tune\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "from ray.rllib.connectors.env_to_module.frame_stacking import FrameStackingEnvToModule\n",
        "from ray.rllib.connectors.learner.frame_stacking import FrameStackingLearner\n",
        "\n",
        "# Can import just yet because Ray RLLIB hasn't updated the pip packages yet\n",
        "from ray.rllib.core.rl_module.default_model_config import DefaultModelConfig\n",
        "from ray.rllib.env.wrappers.atari_wrappers import wrap_atari_for_new_api_stack\n",
        "from ray.rllib.utils.test_utils import add_rllib_example_script_args\n",
        "\n",
        "\n",
        "parser = add_rllib_example_script_args(\n",
        "    default_reward=float(\"inf\"),\n",
        "    default_timesteps=3000000,\n",
        "    default_iters=100000000000,\n",
        ")\n",
        "parser.set_defaults(enable_new_api_stack=True)\n",
        "# Use `parser` to add your own custom command line options to this script\n",
        "# and (if needed) use their values toset up `config` below.\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "def _make_env_to_module_connector(env):\n",
        "    return FrameStackingEnvToModule(num_frames=4)\n",
        "\n",
        "\n",
        "def _make_learner_connector(input_observation_space, input_action_space):\n",
        "    return FrameStackingLearner(num_frames=4)\n",
        "\n",
        "\n",
        "# Create a custom Atari setup (w/o the usual RLlib-hard-coded framestacking in it).\n",
        "# We would like our frame stacking connector to do this job.\n",
        "def _env_creator(cfg):\n",
        "    return wrap_atari_for_new_api_stack(\n",
        "        gym.make(args.env, **cfg, render_mode=\"rgb_array\"),\n",
        "        # Perform frame-stacking through ConnectorV2 API.\n",
        "        framestack=None,\n",
        "    )\n",
        "\n",
        "\n",
        "tune.register_env(\"env\", _env_creator)\n",
        "\n",
        "\n",
        "config = (\n",
        "    PPOConfig()\n",
        "    .environment(\n",
        "        \"env\",\n",
        "        env_config={\n",
        "            # Make analogous to old v4 + NoFrameskip.\n",
        "            \"frameskip\": 1,\n",
        "            \"full_action_space\": False,\n",
        "            \"repeat_action_probability\": 0.0,\n",
        "        },\n",
        "        clip_rewards=True,\n",
        "    )\n",
        "    .env_runners(\n",
        "        # num_envs_per_env_runner=5,  # 5 on old yaml example\n",
        "        env_to_module_connector=_make_env_to_module_connector,\n",
        "    )\n",
        "    .training(\n",
        "        learner_connector=_make_learner_connector,\n",
        "        train_batch_size_per_learner=4000,  # 5000 on old yaml example\n",
        "        minibatch_size=128,  # 500 on old yaml example\n",
        "        lambda_=0.95,\n",
        "        kl_coeff=0.5,\n",
        "        clip_param=0.1,\n",
        "        vf_clip_param=10.0,\n",
        "        entropy_coeff=0.01,\n",
        "        num_epochs=10,\n",
        "        lr=0.00015 * args.num_gpus,\n",
        "        grad_clip=100.0,\n",
        "        grad_clip_by=\"global_norm\",\n",
        "    )\n",
        "    .rl_module(\n",
        "        model_config=DefaultModelConfig(\n",
        "            conv_filters=[[16, 4, 2], [32, 4, 2], [64, 4, 2], [128, 4, 2]],\n",
        "            conv_activation=\"relu\",\n",
        "            head_fcnet_hiddens=[256],\n",
        "            vf_share_layers=True,\n",
        "        ),\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from ray.rllib.utils.test_utils import run_rllib_example_script_experiment\n",
        "\n",
        "    run_rllib_example_script_experiment(config, args=args)"
      ],
      "metadata": {
        "id": "MUpPSfDqnbAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1e15-5pO2s4s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}