{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPk63gnjjQcCct2/qTiegDw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuds/rl-atari-pong/blob/main/%5BAtari%20Tennis%5D%20Multi-Agent%20Reinforcement%20Learning%20PPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Atari Pong] Multi-Agent Reinforcement Learning PPO"
      ],
      "metadata": {
        "id": "ob-2hGT3jNdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig"
      ],
      "metadata": {
        "id": "Fy1WJ1R-iP4V",
        "outputId": "a8c97316-98af-4f94-8809-648d99cd4d3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ray[rllib]"
      ],
      "metadata": {
        "id": "5Lb1vPd-bQvK",
        "outputId": "d98c7c9c-6e55-4a23-b8a0-54850f3a5c32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rlcard\n",
            "  Downloading rlcard-1.2.0.tar.gz (269 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/269.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.0/269.0 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ray[rllib] in /usr/local/lib/python3.10/dist-packages (2.37.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (3.16.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (24.1)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (6.0.2)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (2.2.2)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (16.1.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (2024.6.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (0.1.8)\n",
            "Requirement already satisfied: gymnasium==0.28.1 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (0.28.1)\n",
            "Requirement already satisfied: lz4 in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (4.3.3)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (0.24.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (1.13.1)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (0.12.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from ray[rllib]) (13.9.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (1.26.4)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.28.1->ray[rllib]) (0.0.4)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from rlcard) (2.5.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[rllib]) (0.20.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[rllib]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[rllib]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[rllib]) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[rllib]) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->ray[rllib]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->ray[rllib]) (2.18.0)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (3.3)\n",
            "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (10.4.0)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (2.35.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (2024.9.20)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->ray[rllib]) (0.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer->ray[rllib]) (1.5.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->ray[rllib]) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->ray[rllib]) (1.16.0)\n",
            "Building wheels for collected packages: rlcard\n",
            "  Building wheel for rlcard (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rlcard: filename=rlcard-1.2.0-py3-none-any.whl size=325795 sha256=82739f1fb094b884aa8b55dd512434f1257638fed8401a36e54595e801c8c364\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/0a/39/26d73b035027276e526bec94b0217ed799109d7890c34a7d9b\n",
            "Successfully built rlcard\n",
            "Installing collected packages: rlcard\n",
            "Successfully installed rlcard-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[atari] autorom[accept-rom-license]"
      ],
      "metadata": {
        "id": "9TPt_N0zax24",
        "outputId": "df7c2c02-b2f3-47f8-9274-0e764b353406",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: autorom[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.26.4)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Collecting shimmy<1.0,>=0.1.0 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (2.32.3)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (0.6.1)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (2024.8.30)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (6.4.5)\n",
            "Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ale-py, shimmy\n",
            "  Attempting uninstall: ale-py\n",
            "    Found existing installation: ale-py 0.10.1\n",
            "    Uninstalling ale-py-0.10.1:\n",
            "      Successfully uninstalled ale-py-0.10.1\n",
            "Successfully installed ale-py-0.8.1 shimmy-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pettingzoo[atari]"
      ],
      "metadata": {
        "id": "7U_qTvGlj2oy",
        "outputId": "5aad24db-1910-4176-9377-bdb8594718bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pettingzoo[atari] in /usr/local/lib/python3.10/dist-packages (1.24.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[atari]) (1.26.4)\n",
            "Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo[atari]) (0.28.1)\n",
            "Collecting multi-agent-ale-py==0.1.11 (from pettingzoo[atari])\n",
            "  Downloading multi-agent-ale-py-0.1.11.tar.gz (551 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m552.0/552.0 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.3.0 (from pettingzoo[atari])\n",
            "  Downloading pygame-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[atari]) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo[atari]) (0.0.4)\n",
            "Downloading pygame-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: multi-agent-ale-py\n",
            "  Building wheel for multi-agent-ale-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for multi-agent-ale-py: filename=multi_agent_ale_py-0.1.11-cp310-cp310-linux_x86_64.whl size=721820 sha256=338fadc2f11cdb75bdc634ebd1ff487951881922b8bfb27cf5d87a6bf0c0dc8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/75/64/8ad68adb0da26405c4b18c291b9c322c44d3e99c16b0f3b890\n",
            "Successfully built multi-agent-ale-py\n",
            "Installing collected packages: pygame, multi-agent-ale-py\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.6.1\n",
            "    Uninstalling pygame-2.6.1:\n",
            "      Successfully uninstalled pygame-2.6.1\n",
            "Successfully installed multi-agent-ale-py-0.1.11 pygame-2.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import platform\n",
        "import ray\n",
        "import torch\n",
        "import numpy\n",
        "import gymnasium as gym\n",
        "from pettingzoo.atari import pong_v3\n",
        "from importlib.metadata import version\n",
        "import time\n",
        "\n",
        "import os\n",
        "\n",
        "import ray\n",
        "from gymnasium.spaces import Box, Discrete\n",
        "from ray import tune\n",
        "from ray.rllib.algorithms.dqn import DQNConfig\n",
        "from ray.rllib.algorithms.dqn.dqn_torch_model import DQNTorchModel\n",
        "from ray.rllib.env import PettingZooEnv\n",
        "from ray.rllib.models import ModelCatalog\n",
        "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
        "from ray.rllib.utils.framework import try_import_torch\n",
        "from ray.rllib.utils.torch_utils import FLOAT_MAX\n",
        "from ray.tune.registry import register_env\n",
        "\n",
        "from pettingzoo.classic import leduc_holdem_v4"
      ],
      "metadata": {
        "id": "o89JlEiexh1-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Python Version: {platform.python_version()}\")\n",
        "print(f\"Torch Version: {version('torch')}\")\n",
        "print(f\"Is Cuda Available: {torch.cuda.is_available()}\")\n",
        "print(f\"Cuda Version: {torch.version.cuda}\")\n",
        "print(f\"Gymnasium Version: {version('gymnasium')}\")\n",
        "print(f\"Numpy Version: {version('numpy')}\")\n",
        "print(f\"PettingZoo Version: {version('pettingzoo')}\")\n",
        "print(f\"Ray Version: {version('ray')}\")\n",
        "print(f\"ALE Version: {version('ale_py')}\")"
      ],
      "metadata": {
        "id": "KRrJSPuDiDN4",
        "outputId": "66bd062b-1e32-452b-c912-8de16f5ed7b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python Version: 3.10.12\n",
            "Torch Version: 2.4.1+cu121\n",
            "Is Cuda Available: True\n",
            "Cuda Version: 12.1\n",
            "Gymnasium Version: 0.28.1\n",
            "Numpy Version: 1.26.4\n",
            "PettingZoo Version: 1.24.3\n",
            "Ray Version: 2.37.0\n",
            "ALE Version: 0.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SvPXWTCHiO69"
      },
      "outputs": [],
      "source": [
        "def make_env(env_id):\n",
        "    \"\"\"\n",
        "    Creates and wraps the Atari environment.\n",
        "    \"\"\"\n",
        "    env = tennis_v3.env(render_mode=\"rgb_array\")\n",
        "    env.reset(seed=42)\n",
        "    #env = gym.make(env_id, render_mode='rgb_array')\n",
        "    env = NoopResetEnv(env, noop_max=30)\n",
        "    env = MaxAndSkipEnv(env, skip=4)\n",
        "    env = EpisodicLifeEnv(env)\n",
        "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
        "        env = FireResetEnv(env)\n",
        "    env = WarpFrame(env)\n",
        "    env = ClipRewardEnv(env)\n",
        "    return env\n",
        "\n",
        "def evaluate_agent():\n",
        "    # Create the environment for evaluation\n",
        "    env_id = \"ALE/Tennis-v5\"\n",
        "    env = gym.make(env_id, render_mode='human')\n",
        "\n",
        "    # Apply necessary wrappers\n",
        "\n",
        "    env = MaxAndSkipEnv(env, skip=4)\n",
        "    env = EpisodicLifeEnv(env)\n",
        "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
        "        env = FireResetEnv(env)\n",
        "    env = WarpFrame(env)\n",
        "    env = ClipRewardEnv(env)\n",
        "\n",
        "    # Stack frames\n",
        "    env = DummyVecEnv([lambda: env])\n",
        "    env = VecFrameStack(env, n_stack=4)\n",
        "\n",
        "    # Load the trained model\n",
        "    model = PPO.load(\"ppo_atari_tennis\")\n",
        "\n",
        "    obs = env.reset()\n",
        "    while True:\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, rewards, dones, infos = env.step(action)\n",
        "        # Rendering is handled by the environment when render_mode='human'\n",
        "        if dones:\n",
        "            obs = env.reset()\n",
        "\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment ID for Atari Tennis\n",
        "env_id = \"ALE/Tennis-v5\"\n",
        "\n",
        "# Number of parallel environments (increase for faster training)\n",
        "num_envs = 8  # You can adjust this number\n",
        "\n",
        "# Create the vectorized environment\n",
        "env = make_env(env_id)\n",
        "env = DummyVecEnv([lambda: env for _ in range(num_envs)])\n",
        "\n",
        "# Stack frames (for temporal information)\n",
        "env = VecFrameStack(env, n_stack=4)\n",
        "\n",
        "# Create the PPO agent with CNN policy (since observations are images)\n",
        "model = PPO(\"CnnPolicy\", env, verbose=1)\n",
        "\n",
        "# Train the agent\n",
        "total_timesteps = 10_000_000  # Adjust as needed\n",
        "model.learn(total_timesteps=total_timesteps)\n",
        "\n",
        "# Save the model\n",
        "model.save(\"ppo_atari_tennis\")\n",
        "\n",
        "# Close the environment\n",
        "env.close()\n",
        "\n",
        "# Evaluate the trained agent\n",
        "evaluate_agent()"
      ],
      "metadata": {
        "id": "haCywSvPiY3o",
        "outputId": "f27a819d-5c86-4416-ba98-7c360cfd6d53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "rom tennis is not installed. Please install roms using AutoROM tool (https://github.com/Farama-Foundation/AutoROM) or specify and double-check the path to your Atari rom using the `rom_path` argument.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-92fd8611531c>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Create the vectorized environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0menv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-fe6f5ebab568>\u001b[0m in \u001b[0;36mmake_env\u001b[0;34m(env_id)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mCreates\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mwraps\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mAtari\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtennis_v3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrender_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rgb_array\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#env = gym.make(env_id, render_mode='rgb_array')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pettingzoo/atari/base_atari_env.py\u001b[0m in \u001b[0;36menv_fn\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbase_env_wrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_env_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0menv_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_env_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAssertOutOfBoundsWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOrderEnforcingWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pettingzoo/atari/tennis/tennis.py\u001b[0m in \u001b[0;36mraw_env\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mversion_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mversion_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     return BaseAtariEnv(\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mgame\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tennis\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_players\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pettingzoo/atari/base_atari_env.py\u001b[0m in \u001b[0;36mBaseAtariEnv\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mBaseAtariEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_to_aec_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParallelAtariEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pettingzoo/atari/base_atari_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, game, num_players, mode_num, seed, obs_type, full_action_space, env_name, max_cycles, render_mode, auto_rom_install_path)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             raise OSError(\n\u001b[0m\u001b[1;32m    110\u001b[0m                 \u001b[0;34mf\"rom {game} is not installed. Please install roms using AutoROM tool (https://github.com/Farama-Foundation/AutoROM) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;34m\"or specify and double-check the path to your Atari rom using the `rom_path` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: rom tennis is not installed. Please install roms using AutoROM tool (https://github.com/Farama-Foundation/AutoROM) or specify and double-check the path to your Atari rom using the `rom_path` argument."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pettingzoo.atari import tennis_v3\n",
        "\n",
        "#Environments can be interacted with in a manner very similar to Gymnasium:\n",
        "\n",
        "env.reset()\n",
        "for agent in env.agent_iter():\n",
        "    observation, reward, termination, truncation, info = env.last()\n",
        "    action = None if termination or truncation else env.action_space(agent).sample()  # this is where you would insert your policy\n",
        "    env.step(action)"
      ],
      "metadata": {
        "id": "5Jd9uSXxxeYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = tennis_v3.env()\n",
        "env = NoopResetEnv(env, noop_max=30)"
      ],
      "metadata": {
        "id": "_CjuQ0tCy5AU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Uses Stable-Baselines3 to train agents in the Knights-Archers-Zombies environment using SuperSuit vector envs.\n",
        "\n",
        "This environment requires using SuperSuit's Black Death wrapper, to handle agent death.\n",
        "\n",
        "For more information, see https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html\n",
        "\n",
        "Author: Elliot (https://github.com/elliottower)\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "\n",
        "import supersuit as ss\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.ppo import CnnPolicy, MlpPolicy\n",
        "\n",
        "from pettingzoo.butterfly import knights_archers_zombies_v10\n",
        "\n",
        "\n",
        "def train(env_fn, steps: int = 10_000, seed: int | None = 0, **env_kwargs):\n",
        "    # Train a single model to play as each agent in an AEC environment\n",
        "    env = env_fn.parallel_env(**env_kwargs)\n",
        "\n",
        "    # Add black death wrapper so the number of agents stays constant\n",
        "    # MarkovVectorEnv does not support environments with varying numbers of active agents unless black_death is set to True\n",
        "    env = ss.black_death_v3(env)\n",
        "\n",
        "    # Pre-process using SuperSuit\n",
        "    visual_observation = not env.unwrapped.vector_state\n",
        "    if visual_observation:\n",
        "        # If the observation space is visual, reduce the color channels, resize from 512px to 84px, and apply frame stacking\n",
        "        env = ss.color_reduction_v0(env, mode=\"B\")\n",
        "        env = ss.resize_v1(env, x_size=84, y_size=84)\n",
        "        env = ss.frame_stack_v1(env, 3)\n",
        "\n",
        "    env.reset(seed=seed)\n",
        "\n",
        "    print(f\"Starting training on {str(env.metadata['name'])}.\")\n",
        "\n",
        "    env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
        "    env = ss.concat_vec_envs_v1(env, 8, num_cpus=1, base_class=\"stable_baselines3\")\n",
        "\n",
        "    # Use a CNN policy if the observation space is visual\n",
        "    model = PPO(\n",
        "        CnnPolicy if visual_observation else MlpPolicy,\n",
        "        env,\n",
        "        verbose=3,\n",
        "        batch_size=256,\n",
        "    )\n",
        "\n",
        "    model.learn(total_timesteps=steps)\n",
        "\n",
        "    model.save(f\"{env.unwrapped.metadata.get('name')}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
        "\n",
        "    print(\"Model has been saved.\")\n",
        "\n",
        "    print(f\"Finished training on {str(env.unwrapped.metadata['name'])}.\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "\n",
        "def eval(env_fn, num_games: int = 100, render_mode: str | None = None, **env_kwargs):\n",
        "    # Evaluate a trained agent vs a random agent\n",
        "    env = env_fn.env(render_mode=render_mode, **env_kwargs)\n",
        "\n",
        "    # Pre-process using SuperSuit\n",
        "    visual_observation = not env.unwrapped.vector_state\n",
        "    if visual_observation:\n",
        "        # If the observation space is visual, reduce the color channels, resize from 512px to 84px, and apply frame stacking\n",
        "        env = ss.color_reduction_v0(env, mode=\"B\")\n",
        "        env = ss.resize_v1(env, x_size=84, y_size=84)\n",
        "        env = ss.frame_stack_v1(env, 3)\n",
        "\n",
        "    print(\n",
        "        f\"\\nStarting evaluation on {str(env.metadata['name'])} (num_games={num_games}, render_mode={render_mode})\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        latest_policy = max(\n",
        "            glob.glob(f\"{env.metadata['name']}*.zip\"), key=os.path.getctime\n",
        "        )\n",
        "    except ValueError:\n",
        "        print(\"Policy not found.\")\n",
        "        exit(0)\n",
        "\n",
        "    model = PPO.load(latest_policy)\n",
        "\n",
        "    rewards = {agent: 0 for agent in env.possible_agents}\n",
        "\n",
        "    # Note: we evaluate here using an AEC environments, to allow for easy A/B testing against random policies\n",
        "    # For example, we can see here that using a random agent for archer_0 results in less points than the trained agent\n",
        "    for i in range(num_games):\n",
        "        env.reset(seed=i)\n",
        "        env.action_space(env.possible_agents[0]).seed(i)\n",
        "\n",
        "        for agent in env.agent_iter():\n",
        "            obs, reward, termination, truncation, info = env.last()\n",
        "\n",
        "            for a in env.agents:\n",
        "                rewards[a] += env.rewards[a]\n",
        "\n",
        "            if termination or truncation:\n",
        "                break\n",
        "            else:\n",
        "                if agent == env.possible_agents[0]:\n",
        "                    act = env.action_space(agent).sample()\n",
        "                else:\n",
        "                    act = model.predict(obs, deterministic=True)[0]\n",
        "            env.step(act)\n",
        "    env.close()\n",
        "\n",
        "    avg_reward = sum(rewards.values()) / len(rewards.values())\n",
        "    avg_reward_per_agent = {\n",
        "        agent: rewards[agent] / num_games for agent in env.possible_agents\n",
        "    }\n",
        "    print(f\"Avg reward: {avg_reward}\")\n",
        "    print(\"Avg reward per agent, per game: \", avg_reward_per_agent)\n",
        "    print(\"Full rewards: \", rewards)\n",
        "    return avg_reward\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env_fn = knights_archers_zombies_v10\n",
        "\n",
        "    # Set vector_state to false in order to use visual observations (significantly longer training time)\n",
        "    env_kwargs = dict(max_cycles=100, max_zombies=4, vector_state=True)\n",
        "\n",
        "    # Train a model (takes ~5 minutes on a laptop CPU)\n",
        "    train(env_fn, steps=81_920, seed=0, **env_kwargs)\n",
        "\n",
        "    # Evaluate 10 games (takes ~10 seconds on a laptop CPU)\n",
        "    eval(env_fn, num_games=10, render_mode=None, **env_kwargs)\n",
        "\n",
        "    # Watch 2 games (takes ~10 seconds on a laptop CPU)\n",
        "    eval(env_fn, num_games=2, render_mode=\"human\", **env_kwargs)"
      ],
      "metadata": {
        "id": "QTY7yBHNzsJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import supersuit as ss\n",
        "from pettingzoo.atari import tennis_v3\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import VecMonitor\n",
        "from pettingzoo.utils.conversions import aec_to_parallel\n",
        "\n",
        "# Create the PettingZoo environment\n",
        "env = tennis_v3.env()\n",
        "\n",
        "# Apply Supersuit wrappers to make the environment compatible with Stable Baselines3\n",
        "env = ss.max_observation_v0(env, 2)  # Ensure observations are the same size\n",
        "env = ss.pad_action_space_v0(env)     # Ensure action spaces are the same size\n",
        "env = aec_to_parallel(env)\n",
        "#env = ss.pettingzoo_env_to_vec_env_v1(env)  # Convert to vectorized environment\n",
        "#env = VecMonitor(env)  # Monitor to keep track of rewards and other info\n",
        "\n",
        "# Create the model using the CNN policy for processing image observations\n",
        "model = PPO('CnnPolicy', env, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "model.learn(total_timesteps=500000)\n",
        "\n",
        "# Save the model\n",
        "model.save(\"ppo_tennis_selfplay\")\n",
        "\n",
        "# Load the trained model\n",
        "model = PPO.load(\"ppo_tennis_selfplay\")\n",
        "\n",
        "# Evaluation loop\n",
        "env = tennis_v3.env()\n",
        "env = ss.max_observation_v0(env, 2)\n",
        "env = ss.pad_action_space_v0(env)\n",
        "env.reset()\n",
        "\n",
        "for agent in env.agent_iter():\n",
        "    observation, reward, done, info = env.last()\n",
        "    if done:\n",
        "        action = None\n",
        "    else:\n",
        "        # Use the trained model to predict actions\n",
        "        action, _ = model.predict(observation, deterministic=True)\n",
        "    env.step(action)\n",
        "    env.render()"
      ],
      "metadata": {
        "id": "LH1FW53I0yil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.env import PettingZooEnv\n",
        "from pettingzoo.atari import tennis_v3\n",
        "from ray.tune.registry import register_env\n",
        "from ray.rllib.agents.ppo import PPOTrainer\n",
        "\n",
        "# Initialize Ray\n",
        "ray.init()\n",
        "\n",
        "# Environment creator function\n",
        "def env_creator(config):\n",
        "    env = tennis_v3.env()\n",
        "    return env\n",
        "\n",
        "# Register the environment with RLlib\n",
        "register_env(\"tennis_v3\", lambda config: PettingZooEnv(env_creator(config)))\n",
        "\n",
        "# Create an instance of the environment to extract spaces\n",
        "temp_env = PettingZooEnv(env_creator({}))\n",
        "obs_space = temp_env.observation_space\n",
        "act_space = temp_env.action_space\n",
        "\n",
        "# Define the policies\n",
        "policies = {\n",
        "    \"shared_policy\": (None, obs_space, act_space, {})\n",
        "}\n",
        "\n",
        "# Policy mapping function\n",
        "def policy_mapping_fn(agent_id, episode, **kwargs):\n",
        "    return \"shared_policy\"  # All agents use the same policy (self-play)\n",
        "\n",
        "# RLlib configuration\n",
        "config = {\n",
        "    \"env\": \"tennis_v3\",\n",
        "    \"env_config\": {},\n",
        "    \"framework\": \"torch\",  # Use \"tf\" if you prefer TensorFlow\n",
        "    \"num_workers\": 1,      # Increase if you have more CPUs\n",
        "    \"num_gpus\": 0,         # Set to 1 if you have a GPU\n",
        "    \"multiagent\": {\n",
        "        \"policies\": policies,\n",
        "        \"policy_mapping_fn\": policy_mapping_fn,\n",
        "    },\n",
        "    \"lr\": 1e-4,\n",
        "    \"train_batch_size\": 4000,\n",
        "    \"rollout_fragment_length\": 200,\n",
        "    \"sgd_minibatch_size\": 128,\n",
        "    \"num_sgd_iter\": 10,\n",
        "    \"clip_param\": 0.1,\n",
        "}\n",
        "\n",
        "# Start training\n",
        "results = tune.run(\n",
        "    \"PPO\",\n",
        "    config=config,\n",
        "    stop={\"timesteps_total\": 500000},\n",
        "    checkpoint_at_end=True,\n",
        ")\n",
        "\n",
        "# Get the last checkpoint\n",
        "checkpoints = results.get_trial_checkpoints_paths(\n",
        "    trial=results.get_best_trial(\"episode_reward_mean\", mode=\"max\"),\n",
        "    metric=\"episode_reward_mean\"\n",
        ")\n",
        "checkpoint_path = checkpoints[0][0]\n",
        "\n",
        "# Load the trained agent\n",
        "agent = PPOTrainer(config=config)\n",
        "agent.restore(checkpoint_path)\n",
        "\n",
        "# Evaluation loop\n",
        "env = PettingZooEnv(env_creator({}))\n",
        "env.reset()\n",
        "\n",
        "for agent_id in env.agent_iter():\n",
        "    observation, reward, done, info = env.last()\n",
        "    if done:\n",
        "        action = None\n",
        "    else:\n",
        "        action = agent.compute_single_action(observation, policy_id=\"shared_policy\")\n",
        "    env.step(action)\n",
        "    env.render()\n",
        "\n",
        "# Shutdown Ray\n",
        "ray.shutdown()\n"
      ],
      "metadata": {
        "id": "EL6yKRU6iy7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ray\n",
        "from ray import tune\n",
        "from ray.rllib.env import PettingZooEnv\n",
        "from pettingzoo.atari import tennis_v3\n",
        "from ray.tune.registry import register_env\n",
        "from ray.rllib.algoirthms.ppo import PPOTrainer\n",
        "\n",
        "# Initialize Ray\n",
        "ray.init()\n",
        "\n",
        "# Environment creator function\n",
        "def env_creator(config):\n",
        "    env = tennis_v3.env()\n",
        "    return env\n",
        "\n",
        "# Register the environment with RLlib\n",
        "register_env(\"tennis_v3\", lambda config: PettingZooEnv(env_creator(config)))\n",
        "\n",
        "# Create an instance of the environment to extract spaces\n",
        "temp_env = PettingZooEnv(env_creator({}))\n",
        "obs_space = temp_env.observation_space\n",
        "act_space = temp_env.action_space\n",
        "\n",
        "# Define the policies\n",
        "policies = {\n",
        "    \"shared_policy\": (None, obs_space, act_space, {})\n",
        "}\n",
        "\n",
        "# Policy mapping function\n",
        "def policy_mapping_fn(agent_id, episode, **kwargs):\n",
        "    return \"shared_policy\"  # All agents use the same policy (self-play)\n",
        "\n",
        "# RLlib configuration\n",
        "config = {\n",
        "    \"env\": \"tennis_v3\",\n",
        "    \"env_config\": {},\n",
        "    \"framework\": \"torch\",  # Use \"tf\" if you prefer TensorFlow\n",
        "    \"num_workers\": 1,      # Increase if you have more CPUs\n",
        "    \"num_gpus\": 0,         # Set to 1 if you have a GPU\n",
        "    \"multiagent\": {\n",
        "        \"policies\": policies,\n",
        "        \"policy_mapping_fn\": policy_mapping_fn,\n",
        "    },\n",
        "    \"lr\": 1e-4,\n",
        "    \"train_batch_size\": 4000,\n",
        "    \"rollout_fragment_length\": 200,\n",
        "    \"sgd_minibatch_size\": 128,\n",
        "    \"num_sgd_iter\": 10,\n",
        "    \"clip_param\": 0.1,\n",
        "}\n",
        "\n",
        "# Start training\n",
        "results = tune.run(\n",
        "    \"PPO\",\n",
        "    config=config,\n",
        "    stop={\"timesteps_total\": 500000},\n",
        "    checkpoint_at_end=True,\n",
        ")\n",
        "\n",
        "# Get the last checkpoint\n",
        "checkpoints = results.get_trial_checkpoints_paths(\n",
        "    trial=results.get_best_trial(\"episode_reward_mean\", mode=\"max\"),\n",
        "    metric=\"episode_reward_mean\"\n",
        ")\n",
        "checkpoint_path = checkpoints[0][0]\n",
        "\n",
        "# Load the trained agent\n",
        "agent = PPOTrainer(config=config)\n",
        "agent.restore(checkpoint_path)\n",
        "\n",
        "# Evaluation loop\n",
        "env = PettingZooEnv(env_creator({}))\n",
        "env.reset()\n",
        "\n",
        "for agent_id in env.agent_iter():\n",
        "    observation, reward, done, info = env.last()\n",
        "    if done:\n",
        "        action = None\n",
        "    else:\n",
        "        action = agent.compute_single_action(observation, policy_id=\"shared_policy\")\n",
        "    env.step(action)\n",
        "    env.render()\n",
        "\n",
        "# Shutdown Ray\n",
        "ray.shutdown()\n"
      ],
      "metadata": {
        "id": "Nn4YW58rlwts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Uses Ray's RLlib to train agents to play Pistonball.\n",
        "\n",
        "Author: Rohan (https://github.com/Rohan138)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "\n",
        "import ray\n",
        "import supersuit as ss\n",
        "from ray import tune\n",
        "from ray.rllib.algorithms.ppo import PPOConfig\n",
        "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
        "from ray.rllib.models import ModelCatalog\n",
        "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
        "from ray.tune.registry import register_env\n",
        "from torch import nn\n",
        "\n",
        "from pettingzoo.butterfly import pistonball_v6\n",
        "\n",
        "\n",
        "class CNNModelV2(TorchModelV2, nn.Module):\n",
        "    def __init__(self, obs_space, act_space, num_outputs, *args, **kwargs):\n",
        "        TorchModelV2.__init__(self, obs_space, act_space, num_outputs, *args, **kwargs)\n",
        "        nn.Module.__init__(self)\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, [8, 8], stride=(4, 4)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, [4, 4], stride=(2, 2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, [3, 3], stride=(1, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            (nn.Linear(3136, 512)),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.policy_fn = nn.Linear(512, num_outputs)\n",
        "        self.value_fn = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, input_dict, state, seq_lens):\n",
        "        model_out = self.model(input_dict[\"obs\"].permute(0, 3, 1, 2))\n",
        "        self._value_out = self.value_fn(model_out)\n",
        "        return self.policy_fn(model_out), state\n",
        "\n",
        "    def value_function(self):\n",
        "        return self._value_out.flatten()\n",
        "\n",
        "\n",
        "def env_creator(args):\n",
        "    env = pistonball_v6.parallel_env(\n",
        "        n_pistons=20,\n",
        "        time_penalty=-0.1,\n",
        "        continuous=True,\n",
        "        random_drop=True,\n",
        "        random_rotate=True,\n",
        "        ball_mass=0.75,\n",
        "        ball_friction=0.3,\n",
        "        ball_elasticity=1.5,\n",
        "        max_cycles=125,\n",
        "    )\n",
        "    env = ss.color_reduction_v0(env, mode=\"B\")\n",
        "    env = ss.dtype_v0(env, \"float32\")\n",
        "    env = ss.resize_v1(env, x_size=84, y_size=84)\n",
        "    env = ss.normalize_obs_v0(env, env_min=0, env_max=1)\n",
        "    env = ss.frame_stack_v1(env, 3)\n",
        "    return env\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     ray.init()\n",
        "\n",
        "#     env_name = \"pistonball_v6\"\n",
        "\n",
        "#     register_env(env_name, lambda config: ParallelPettingZooEnv(env_creator(config)))\n",
        "#     ModelCatalog.register_custom_model(\"CNNModelV2\", CNNModelV2)\n",
        "\n",
        "#     config = (\n",
        "#         PPOConfig()\n",
        "#         .environment(env=env_name, clip_actions=True)\n",
        "#         .rollouts(num_rollout_workers=4, rollout_fragment_length=128)\n",
        "#         .training(\n",
        "#             train_batch_size=512,\n",
        "#             lr=2e-5,\n",
        "#             gamma=0.99,\n",
        "#             lambda_=0.9,\n",
        "#             use_gae=True,\n",
        "#             clip_param=0.4,\n",
        "#             grad_clip=None,\n",
        "#             entropy_coeff=0.1,\n",
        "#             vf_loss_coeff=0.25,\n",
        "#             sgd_minibatch_size=64,\n",
        "#             num_sgd_iter=10,\n",
        "#         )\n",
        "#         .debugging(log_level=\"ERROR\")\n",
        "#         .framework(framework=\"torch\")\n",
        "#         .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
        "#     )\n",
        "\n",
        "#     tune.run(\n",
        "#         \"PPO\",\n",
        "#         name=\"PPO\",\n",
        "#         stop={\"timesteps_total\": 5000000 if not os.environ.get(\"CI\") else 50000},\n",
        "#         checkpoint_freq=10,\n",
        "#         storage_path=\"~/ray_results/\" + env_name,\n",
        "#         config=config.to_dict(),\n",
        "#     )"
      ],
      "metadata": {
        "id": "fsA0mDWpmO63",
        "outputId": "c7cef102-47a4-4f4d-b4d7-973ca1936c31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.__version__) <\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "torch, nn = try_import_torch()\n",
        "\n",
        "\n",
        "class TorchMaskedActions(DQNTorchModel):\n",
        "    \"\"\"PyTorch version of above ParametricActionsModel.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        obs_space: Box,\n",
        "        action_space: Discrete,\n",
        "        num_outputs,\n",
        "        model_config,\n",
        "        name,\n",
        "        **kw,\n",
        "    ):\n",
        "        DQNTorchModel.__init__(\n",
        "            self, obs_space, action_space, num_outputs, model_config, name, **kw\n",
        "        )\n",
        "\n",
        "        obs_len = obs_space.shape[0] - action_space.n\n",
        "\n",
        "        orig_obs_space = Box(\n",
        "            shape=(obs_len,), low=obs_space.low[:obs_len], high=obs_space.high[:obs_len]\n",
        "        )\n",
        "        self.action_embed_model = TorchFC(\n",
        "            orig_obs_space,\n",
        "            action_space,\n",
        "            action_space.n,\n",
        "            model_config,\n",
        "            name + \"_action_embed\",\n",
        "        )\n",
        "\n",
        "    def forward(self, input_dict, state, seq_lens):\n",
        "        # Extract the available actions tensor from the observation.\n",
        "        action_mask = input_dict[\"obs\"][\"action_mask\"]\n",
        "\n",
        "        # Compute the predicted action embedding\n",
        "        action_logits, _ = self.action_embed_model(\n",
        "            {\"obs\": input_dict[\"obs\"][\"observation\"]}\n",
        "        )\n",
        "        # turns probit action mask into logit action mask\n",
        "        inf_mask = torch.clamp(torch.log(action_mask), -1e10, FLOAT_MAX)\n",
        "\n",
        "        return action_logits + inf_mask, state\n",
        "\n",
        "    def value_function(self):\n",
        "        return self.action_embed_model.value_function()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ray.init()\n",
        "\n",
        "    alg_name = \"DQN\"\n",
        "    ModelCatalog.register_custom_model(\"pa_model\", TorchMaskedActions)\n",
        "    # function that outputs the environment you wish to register.\n",
        "\n",
        "    def env_creator():\n",
        "        env = leduc_holdem_v4.env()\n",
        "        return env\n",
        "\n",
        "    env_name = \"leduc_holdem_v4\"\n",
        "    register_env(env_name, lambda config: PettingZooEnv(env_creator()))\n",
        "\n",
        "    test_env = PettingZooEnv(env_creator())\n",
        "    obs_space = test_env.observation_space\n",
        "    act_space = test_env.action_space\n",
        "\n",
        "    config = (\n",
        "        DQNConfig()\n",
        "        .environment(env=env_name)\n",
        "        .rollouts(num_rollout_workers=1, rollout_fragment_length=30)\n",
        "        .training(\n",
        "            train_batch_size=200,\n",
        "            hiddens=[],\n",
        "            dueling=False,\n",
        "            model={\"custom_model\": \"pa_model\"},\n",
        "        )\n",
        "        .multi_agent(\n",
        "            policies={\n",
        "                \"player_0\": (None, obs_space, act_space, {}),\n",
        "                \"player_1\": (None, obs_space, act_space, {}),\n",
        "            },\n",
        "            policy_mapping_fn=(lambda agent_id, *args, **kwargs: agent_id),\n",
        "        )\n",
        "        .resources(num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")))\n",
        "        .debugging(\n",
        "            log_level=\"DEBUG\"\n",
        "        )  # TODO: change to ERROR to match pistonball example\n",
        "        .framework(framework=\"torch\")\n",
        "        .exploration(\n",
        "            exploration_config={\n",
        "                # The Exploration class to use.\n",
        "                \"type\": \"EpsilonGreedy\",\n",
        "                # Config for the Exploration class' constructor:\n",
        "                \"initial_epsilon\": 0.1,\n",
        "                \"final_epsilon\": 0.0,\n",
        "                \"epsilon_timesteps\": 100000,  # Timesteps over which to anneal epsilon.\n",
        "            }\n",
        "        )\n",
        "    )\n",
        "\n",
        "    tune.run(\n",
        "        alg_name,\n",
        "        name=\"DQN\",\n",
        "        stop={\"timesteps_total\": 10000000 if not os.environ.get(\"CI\") else 50000},\n",
        "        checkpoint_freq=10,\n",
        "        config=config.to_dict(),\n",
        "    )"
      ],
      "metadata": {
        "id": "ZH-ei03GnEtM",
        "outputId": "67546aa8-bfaa-4024-d76b-a6b5e2b2caf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Maybe you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-e21d7bc7ce8f>\u001b[0m in \u001b[0;36m<cell line: 66>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0malg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"DQN\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(address, num_cpus, num_gpus, resources, labels, object_store_memory, local_mode, ignore_reinit_error, include_dashboard, dashboard_host, dashboard_port, job_config, configure_logging, logging_level, logging_format, logging_config, log_to_driver, namespace, runtime_env, storage, **kwargs)\u001b[0m\n\u001b[1;32m   1621\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mRayContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_global_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1623\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1624\u001b[0m                 \u001b[0;34m\"Maybe you called ray.init twice by accident? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1625\u001b[0m                 \u001b[0;34m\"This error can be suppressed by passing in \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Maybe you called ray.init twice by accident? This error can be suppressed by passing in 'ignore_reinit_error=True' or by calling 'ray.shutdown()' prior to 'ray.init()'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/ray-project/ray/issues/16425"
      ],
      "metadata": {
        "id": "CFo4Ls-WuT57"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}